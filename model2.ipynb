{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch pandas rank-bm25 sentence-transformers playsound gtts pickle-mixin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import pandas as pd\n",
    "from rank_bm25 import BM25Okapi\n",
    "import pickle\n",
    "from gtts import gTTS\n",
    "from playsound import playsound\n",
    "import time\n",
    "\n",
    "# Helper function for progress updates\n",
    "def update_checkpoint(message):\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] {message}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_or_load_bm25(dataset_path, cache_path=\"bm25_cache.pkl\"):\n",
    "    try:\n",
    "        # Try loading cached BM25 model and tokenized titles\n",
    "        with open(cache_path, \"rb\") as f:\n",
    "            bm25, df = pickle.load(f)\n",
    "        update_checkpoint(\"Loaded BM25 model from cache.\")\n",
    "    except FileNotFoundError:\n",
    "        # If no cache is found, build BM25 from scratch and save it\n",
    "        update_checkpoint(\"Building BM25 model from scratch...\")\n",
    "        df = pd.read_csv(dataset_path, low_memory=False)\n",
    "        df['Title'] = df['Title'].astype(str).fillna(\"\")\n",
    "        tokenized_titles = [title.split(\" \") for title in df['Title'] if isinstance(title, str)]\n",
    "        bm25 = BM25Okapi(tokenized_titles)\n",
    "        # Cache the BM25 model and tokenized titles for future use\n",
    "        with open(cache_path, \"wb\") as f:\n",
    "            pickle.dump((bm25, df), f)\n",
    "        update_checkpoint(\"BM25 model built and saved to cache.\")\n",
    "    return bm25, df\n",
    "\n",
    "def retrieve_answer(query, bm25, df):\n",
    "    try:\n",
    "        update_checkpoint(\"Searching for matching title...\")\n",
    "        query_tokens = query.split(\" \")\n",
    "        top_n_titles = bm25.get_top_n(query_tokens, df['Title'], n=1)  # Find the closest matching title\n",
    "\n",
    "        if len(top_n_titles) > 0:\n",
    "            # Retrieve the answer corresponding to the top title match\n",
    "            matched_title = top_n_titles[0]\n",
    "            answer = df[df['Title'] == matched_title]['Answer'].values[0]\n",
    "            update_checkpoint(f\"Match found: {matched_title}. Returning answer.\")\n",
    "            return answer\n",
    "        else:\n",
    "            update_checkpoint(\"No matching title found.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        update_checkpoint(f\"Error during retrieval: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_gpt2(query, max_length=150):\n",
    "    try:\n",
    "        update_checkpoint(\"Generating new answer using GPT-2...\")\n",
    "        # Load GPT-2 model and tokenizer\n",
    "        model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "        # Encode the query\n",
    "        inputs = tokenizer.encode(query, return_tensors=\"pt\")\n",
    "\n",
    "        # Generate text based on the query\n",
    "        outputs = model.generate(inputs, max_length=max_length, do_sample=True, temperature=0.7)\n",
    "\n",
    "        # Decode the generated text\n",
    "        generated_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        update_checkpoint(\"New answer generated successfully.\")\n",
    "        return generated_answer\n",
    "    except Exception as e:\n",
    "        update_checkpoint(f\"Error during answer generation: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(text, filename=\"output.mp3\"):\n",
    "    try:\n",
    "        update_checkpoint(\"Converting answer to speech...\")\n",
    "        speech = gTTS(text=text, lang='en', slow=False)\n",
    "        speech.save(filename)\n",
    "        update_checkpoint(f\"Speech saved as '{filename}'.\")\n",
    "\n",
    "        # Play the mp3 file\n",
    "        playsound(filename)\n",
    "    except Exception as e:\n",
    "        update_checkpoint(f\"TTS error: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:13:46] Loaded BM25 model from cache.\n",
      "[08:13:46] Searching for matching title...\n",
      "[08:13:48] Match found: EU affirms 'credible poll' conditions'. Returning answer.\n",
      "[08:13:48] Retrieved answer: Dhaka, Dec 29 (bdnews24.com)--Head of the European Union election observation mission (EOM) Alexander Graf Lambdorff Monday said \"conditions for a credible poll\" were in Bangladesh. After witnessing some polling stations and vote counting at Dhaka University, a satisfied Lambdorff said if the polling across the country was like that of the centres he visited, the 150 member EOM would be happy. \"I have followed the process indeed from morning to evening and our team will continue to follow even after the counting when the results will be transferred to the returning officers,\" Lambdorff told reporters watching vote counting at Curzon Hall at Dhaka University. \"I think that what we have seen on polling day so far has shown that the conditions for a credible poll in Bangladesh are in place,\" he said. He said his team would be collecting information from the members of the EOM from across the country and make a preliminary statement on 31 December. \"What I have seen personally as one of the 150 European observers today has given me a degree of satisfaction and hope that the voting process will continue like that. \"As far as today is concerned, I believe what I have seen at least has drawn satisfactory. And I hope if this is the case across the country, the reports that I have had around mid day today from our team showed that they were satisfied,\" he said. Lambsdorff went to Shuritulla Model School centre and Ispahani High School in Keraniganj to see the transparency of the voting process. bdn\n",
      "[08:13:48] Converting answer to speech...\n",
      "[08:14:02] Speech saved as 'output.mp3'.\n"
     ]
    }
   ],
   "source": [
    "def main_pipeline():\n",
    "    # Step 1: Capture Query Input\n",
    "    query_text = input(\"Enter your query (title): \")\n",
    "\n",
    "    if query_text:\n",
    "        # Step 2: Retrieve Relevant Answer from Dataset\n",
    "        bm25, df = build_or_load_bm25(\"data\\\\new_csv_file.csv\")\n",
    "        retrieved_answer = retrieve_answer(query_text, bm25, df)\n",
    "\n",
    "        # Step 3: If no answer is found, generate a new answer\n",
    "        if retrieved_answer:\n",
    "            answer = retrieved_answer\n",
    "            update_checkpoint(f\"Retrieved answer: {answer}\")\n",
    "        else:\n",
    "            answer = generate_answer_gpt2(query_text, max_length=300)  # Generate a longer answer\n",
    "            update_checkpoint(f\"Generated answer: {answer}\")\n",
    "\n",
    "        if answer:\n",
    "            # Step 4: Optionally convert the answer to speech\n",
    "            text_to_speech(answer)\n",
    "\n",
    "# Run the full pipeline\n",
    "main_pipeline()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

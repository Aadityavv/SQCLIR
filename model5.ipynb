{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:33:46] Loading GPT-2 model and tokenizer...\n",
      "[01:33:50] GPT-2 model and tokenizer loaded successfully.\n",
      "[01:34:11] Loaded BM25 model from cache.\n",
      "[01:34:11] Searching for matching title...\n",
      "[01:34:15] Match found: Political implications of Manmohan Singh's ill-health. Returning context.\n",
      "[01:34:15] Generating new answer using GPT-2...\n",
      "[01:34:15] Error during answer generation: Input length of input_ids is 512, but `max_length` is set to 300. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "[01:34:15] Generated answer: None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 179\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# Run the full pipeline\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 179\u001b[0m     \u001b[43mmain_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 171\u001b[0m, in \u001b[0;36mmain_pipeline\u001b[1;34m()\u001b[0m\n\u001b[0;32m    168\u001b[0m update_checkpoint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated answer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerated_answer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# Step 5: Calculate relevance of the generated answer\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[43mcalculate_relevance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerated_answer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# Step 6: Optionally convert the answer to speech\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generated_answer:\n",
      "Cell \u001b[1;32mIn[1], line 133\u001b[0m, in \u001b[0;36mcalculate_relevance\u001b[1;34m(query, generated_answer)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_relevance\u001b[39m(query, generated_answer):\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# Calculate token overlap as a simple relevance metric\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m \u001b[43mCountVectorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerated_answer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m     vectors \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform([query, generated_answer])\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# Calculate cosine similarity between the query and generated answer vectors\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1325\u001b[0m, in \u001b[0;36mCountVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_documents, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\u001b[39;00m\n\u001b[0;32m   1311\u001b[0m \n\u001b[0;32m   1312\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1323\u001b[0m \u001b[38;5;124;03m        Fitted vectorizer.\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1374\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1366\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m             )\n\u001b[0;32m   1372\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1374\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1377\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1261\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1260\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1261\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1262\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1263\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 68\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import pandas as pd\n",
    "from rank_bm25 import BM25Okapi\n",
    "import pickle\n",
    "from gtts import gTTS\n",
    "from playsound import playsound\n",
    "import time\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Suppress the specific FutureWarning from transformers\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=(\n",
    "        \"`clean_up_tokenization_spaces` was not set. It will be set to `True` by default.\"\n",
    "    ),\n",
    "    category=FutureWarning,\n",
    ")\n",
    "\n",
    "# Helper function for progress updates\n",
    "def update_checkpoint(message):\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] {message}\")\n",
    "\n",
    "def build_or_load_bm25(dataset_path, cache_path=\"bm25_cache.pkl\"):\n",
    "    try:\n",
    "        # Try loading cached BM25 model and tokenized titles\n",
    "        with open(cache_path, \"rb\") as f:\n",
    "            bm25, df = pickle.load(f)\n",
    "        update_checkpoint(\"Loaded BM25 model from cache.\")\n",
    "    except FileNotFoundError:\n",
    "        # If no cache is found, build BM25 from scratch and save it\n",
    "        update_checkpoint(\"Building BM25 model from scratch...\")\n",
    "        df = pd.read_csv(dataset_path, low_memory=False)\n",
    "        df['Title'] = df['Title'].astype(str).fillna(\"\")\n",
    "        tokenized_titles = [title.split(\" \") for title in df['Title'] if isinstance(title, str)]\n",
    "        bm25 = BM25Okapi(tokenized_titles)\n",
    "        # Cache the BM25 model and tokenized titles for future use\n",
    "        with open(cache_path, \"wb\") as f:\n",
    "            pickle.dump((bm25, df), f)\n",
    "        update_checkpoint(\"BM25 model built and saved to cache.\")\n",
    "    return bm25, df\n",
    "\n",
    "def retrieve_answer(query, bm25, df):\n",
    "    try:\n",
    "        update_checkpoint(\"Searching for matching title...\")\n",
    "        query_tokens = query.split(\" \")\n",
    "        top_n_titles = bm25.get_top_n(query_tokens, df['Title'], n=1)  # Find the closest matching title\n",
    "\n",
    "        if len(top_n_titles) > 0:\n",
    "            # Retrieve the answer corresponding to the top title match\n",
    "            matched_title = top_n_titles[0]\n",
    "            context = df[df['Title'] == matched_title]['Answer'].values[0]\n",
    "            update_checkpoint(f\"Match found: {matched_title}. Returning context.\")\n",
    "            return context\n",
    "        else:\n",
    "            update_checkpoint(\"No matching title found.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        update_checkpoint(f\"Error during retrieval: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Load the model and tokenizer once at the start\n",
    "def load_model_and_tokenizer():\n",
    "    try:\n",
    "        update_checkpoint(\"Loading GPT-2 model and tokenizer...\")\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "        # Assign eos_token as pad_token\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        # Resize model embeddings if pad_token is added\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        update_checkpoint(\"GPT-2 model and tokenizer loaded successfully.\")\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        update_checkpoint(f\"Error loading model/tokenizer: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# Load the model and tokenizer globally to avoid reloading every time\n",
    "MODEL, TOKENIZER = load_model_and_tokenizer()\n",
    "\n",
    "def generate_answer_gpt2(query, context, max_length=150):\n",
    "    try:\n",
    "        update_checkpoint(\"Generating new answer using GPT-2...\")\n",
    "\n",
    "        if MODEL is None or TOKENIZER is None:\n",
    "            update_checkpoint(\"Model or tokenizer not loaded. Cannot generate answer.\")\n",
    "            return None\n",
    "\n",
    "        # Combine query and context for input\n",
    "        combined_input = f\"Question: {query}\\nContext: {context}\\nAnswer:\"\n",
    "\n",
    "        # Encode the combined input with padding and truncation\n",
    "        inputs = TOKENIZER(\n",
    "            combined_input,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512  # GPT-2's maximum context size\n",
    "        )\n",
    "\n",
    "        # Generate text based on the query with advanced parameters\n",
    "        outputs = MODEL.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,  # Adjusted temperature for more control\n",
    "            top_k=50,  # Restrict model to the top 50 likely words\n",
    "            top_p=0.9,  # Nucleus sampling for diverse output\n",
    "            repetition_penalty=1.2,  # Discourage repetition\n",
    "            pad_token_id=TOKENIZER.eos_token_id  # Ensure pad_token_id is set\n",
    "        )\n",
    "\n",
    "        # Decode the generated text with clean_up_tokenization_spaces explicitly set\n",
    "        generated_answer = TOKENIZER.decode(\n",
    "            outputs[0],\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True  # Set to True or False based on preference\n",
    "        )\n",
    "        update_checkpoint(\"New answer generated successfully.\")\n",
    "        return generated_answer\n",
    "    except Exception as e:\n",
    "        update_checkpoint(f\"Error during answer generation: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def calculate_relevance(query, generated_answer):\n",
    "    # Calculate token overlap as a simple relevance metric\n",
    "    vectorizer = CountVectorizer().fit([query, generated_answer])\n",
    "    vectors = vectorizer.transform([query, generated_answer]).toarray()\n",
    "\n",
    "    # Calculate cosine similarity between the query and generated answer vectors\n",
    "    cosine_similarity = (vectors[0] @ vectors[1]) / (torch.norm(torch.tensor(vectors[0])) * torch.norm(torch.tensor(vectors[1])))\n",
    "\n",
    "    relevance_percentage = round(cosine_similarity.item() * 100, 2)\n",
    "    print(f\"Relevance of the answer: {relevance_percentage}%\")\n",
    "    return relevance_percentage\n",
    "\n",
    "def text_to_speech(text, filename=\"output.mp3\"):\n",
    "    try:\n",
    "        update_checkpoint(\"Converting answer to speech...\")\n",
    "        speech = gTTS(text=text, lang='en', slow=False)\n",
    "        speech.save(filename)\n",
    "        update_checkpoint(f\"Speech saved as '{filename}'.\")\n",
    "\n",
    "        # Play the mp3 file\n",
    "        playsound(filename)\n",
    "    except Exception as e:\n",
    "        update_checkpoint(f\"TTS error: {str(e)}\")\n",
    "\n",
    "def main_pipeline():\n",
    "    # Step 1: Capture Query Input\n",
    "    query_text = input(\"Enter your query (title): \")\n",
    "\n",
    "    if query_text:\n",
    "        # Step 2: Retrieve Relevant Context from Dataset\n",
    "        bm25, df = build_or_load_bm25(\"data\\\\new_csv_file.csv\")\n",
    "        context = retrieve_answer(query_text, bm25, df)\n",
    "\n",
    "        # Step 3: If no context is found, skip generation\n",
    "        if context:\n",
    "            # Step 4: Generate a new answer based on context and query\n",
    "            generated_answer = generate_answer_gpt2(query_text, context, max_length=300)\n",
    "            update_checkpoint(f\"Generated answer: {generated_answer}\")\n",
    "\n",
    "            # Step 5: Calculate relevance of the generated answer\n",
    "            calculate_relevance(query_text, generated_answer)\n",
    "\n",
    "            # Step 6: Optionally convert the answer to speech\n",
    "            if generated_answer:\n",
    "                text_to_speech(generated_answer)\n",
    "\n",
    "# Run the full pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    main_pipeline()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
